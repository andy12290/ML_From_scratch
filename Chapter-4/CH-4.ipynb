{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classification Accuracy\n",
    "* Accuracy = TP+TN/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_accuracy(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual))*100    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "accuracy = calc_accuracy(actual,predicted)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy starts to lose it's meaning when you have\n",
    "more class values and you may need to review a di\u000b",
    "erent perspective on the results, such as a\n",
    "confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define confusion matrix\n",
    "def confusion_matrix(actual, predicted):\n",
    "    unique = set(actual)\n",
    "    print(\"unique_values\", unique)\n",
    "    matrix = [list() for x in range(len(unique))]\n",
    "    print(\"first check in for matrix\", matrix)\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for x in range(len(unique))]\n",
    "        print(\"zero init\",matrix)\n",
    "    lookup = dict()\n",
    "    print(\"first loopup check in\", lookup)\n",
    "    for i, value in enumerate(unique):\n",
    "        print(\"i\", i)\n",
    "        print(\"y\", value)\n",
    "        lookup[value] = i\n",
    "        print(\" final lookup\", lookup)\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        print(\"actual-x\", x)\n",
    "        y = lookup[predicted[i]]\n",
    "        print(\"predicted-y\", y)\n",
    "        matrix[x][y] += 1\n",
    "        print(\"matrix\", matrix[x][y])\n",
    "    return unique, matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_values {0, 1}\n",
      "first check in for matrix [[], []]\n",
      "zero init [[0, 0], []]\n",
      "zero init [[0, 0], [0, 0]]\n",
      "first loopup check in {}\n",
      "i 0\n",
      "y 0\n",
      " final lookup {0: 0}\n",
      "i 1\n",
      "y 1\n",
      " final lookup {0: 0, 1: 1}\n",
      "actual-x 0\n",
      "predicted-y 0\n",
      "1\n",
      "[[1, 0], [0, 0]] matrix\n",
      "actual-x 0\n",
      "predicted-y 1\n",
      "1\n",
      "[[1, 1], [0, 0]] matrix\n",
      "actual-x 0\n",
      "predicted-y 1\n",
      "2\n",
      "[[1, 2], [0, 0]] matrix\n",
      "actual-x 0\n",
      "predicted-y 0\n",
      "2\n",
      "[[2, 2], [0, 0]] matrix\n",
      "actual-x 0\n",
      "predicted-y 0\n",
      "3\n",
      "[[3, 2], [0, 0]] matrix\n",
      "actual-x 1\n",
      "predicted-y 1\n",
      "1\n",
      "[[3, 2], [0, 1]] matrix\n",
      "actual-x 1\n",
      "predicted-y 0\n",
      "1\n",
      "[[3, 2], [1, 1]] matrix\n",
      "actual-x 1\n",
      "predicted-y 1\n",
      "2\n",
      "[[3, 2], [1, 2]] matrix\n",
      "actual-x 1\n",
      "predicted-y 1\n",
      "3\n",
      "[[3, 2], [1, 3]] matrix\n",
      "actual-x 1\n",
      "predicted-y 1\n",
      "4\n",
      "[[3, 2], [1, 4]] matrix\n",
      "{0, 1}\n",
      "[[3, 2], [1, 4]]\n"
     ]
    }
   ],
   "source": [
    "actual = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,1,0,0,1,0,1,1,1]\n",
    "unique, matrix = confusion_matrix(actual, predicted)\n",
    "print(unique)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mae_metric(actual, predicted):\n",
    "    sum_error = 0\n",
    "    for i in range(len(actual)):\n",
    "        sum_error += abs(predicted[i] - actual[i])\n",
    "        mae_error = sum_error/(len(actual))\n",
    "        print(\"mae\", mae_error)\n",
    "    return mae_error    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mae 0.001999999999999999\n",
      "mae 0.004000000000000001\n",
      "mae 0.006000000000000003\n",
      "mae 0.007999999999999993\n",
      "mae 0.007999999999999993\n",
      "0.007999999999999993\n"
     ]
    }
   ],
   "source": [
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "mae = mae_metric(actual, predicted)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00894427190999915\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "        mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    "# Test RMSE\n",
    "actual = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "predicted = [0.11, 0.19, 0.29, 0.41, 0.5]\n",
    "rmse = rmse_metric(actual, predicted)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
